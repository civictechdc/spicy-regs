{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docket-Level Analysis\n",
    "\n",
    "**Problem**: How can we build clear, digestible summaries and insights from tens of thousands of comments within a single docket?\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Comment volume and timeline analysis\n",
    "- Top themes and keywords extraction\n",
    "- Commenter type breakdown\n",
    "- Sentiment distribution estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ready\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "R2_BASE_URL = \"https://pub-5fc11ad134984edf8d9af452dd1849d6.r2.dev\"\n",
    "\n",
    "conn = duckdb.connect()\n",
    "conn.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "print(\"✓ Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docket_id</th>\n",
       "      <th>agency_code</th>\n",
       "      <th>title</th>\n",
       "      <th>docket_type</th>\n",
       "      <th>modify_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2024-05-20T13:28:23Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2023-08-02T14:59:24Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2023-08-15T14:08:58Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2023-04-13T16:53:27Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2023-12-07T08:19:59Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2024-05-07T15:36:23Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2023-05-09T11:05:01Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2023-04-10T14:41:11Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2024-03-29T16:27:25Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2023-03-28T07:42:11Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2023-07-11T08:35:41Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2024-03-08T12:07:44Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2023-07-20T13:37:09Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2024-05-09T15:36:36Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2023-12-04T13:48:14Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2023-06-08T16:21:33Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2023-12-19T14:28:41Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2023-07-27T09:02:32Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2023-07-13T15:21:58Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2025-07-23T17:07:06Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2023-08-24T15:27:21Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>EPA-HQ-OAR-2021-0317</td>\n",
       "      <td>EPA</td>\n",
       "      <td>Standards of Performance for New, Reconstructe...</td>\n",
       "      <td>Rulemaking</td>\n",
       "      <td>2024-05-15T07:16:39Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               docket_id agency_code  \\\n",
       "0   EPA-HQ-OAR-2021-0317         EPA   \n",
       "1   EPA-HQ-OAR-2021-0317         EPA   \n",
       "2   EPA-HQ-OAR-2021-0317         EPA   \n",
       "3   EPA-HQ-OAR-2021-0317         EPA   \n",
       "4   EPA-HQ-OAR-2021-0317         EPA   \n",
       "5   EPA-HQ-OAR-2021-0317         EPA   \n",
       "6   EPA-HQ-OAR-2021-0317         EPA   \n",
       "7   EPA-HQ-OAR-2021-0317         EPA   \n",
       "8   EPA-HQ-OAR-2021-0317         EPA   \n",
       "9   EPA-HQ-OAR-2021-0317         EPA   \n",
       "10  EPA-HQ-OAR-2021-0317         EPA   \n",
       "11  EPA-HQ-OAR-2021-0317         EPA   \n",
       "12  EPA-HQ-OAR-2021-0317         EPA   \n",
       "13  EPA-HQ-OAR-2021-0317         EPA   \n",
       "14  EPA-HQ-OAR-2021-0317         EPA   \n",
       "15  EPA-HQ-OAR-2021-0317         EPA   \n",
       "16  EPA-HQ-OAR-2021-0317         EPA   \n",
       "17  EPA-HQ-OAR-2021-0317         EPA   \n",
       "18  EPA-HQ-OAR-2021-0317         EPA   \n",
       "19  EPA-HQ-OAR-2021-0317         EPA   \n",
       "20  EPA-HQ-OAR-2021-0317         EPA   \n",
       "21  EPA-HQ-OAR-2021-0317         EPA   \n",
       "\n",
       "                                                title docket_type  \\\n",
       "0   Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "1   Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "2   Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "3   Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "4   Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "5   Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "6   Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "7   Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "8   Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "9   Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "10  Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "11  Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "12  Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "13  Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "14  Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "15  Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "16  Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "17  Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "18  Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "19  Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "20  Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "21  Standards of Performance for New, Reconstructe...  Rulemaking   \n",
       "\n",
       "             modify_date  \n",
       "0   2024-05-20T13:28:23Z  \n",
       "1   2023-08-02T14:59:24Z  \n",
       "2   2023-08-15T14:08:58Z  \n",
       "3   2023-04-13T16:53:27Z  \n",
       "4   2023-12-07T08:19:59Z  \n",
       "5   2024-05-07T15:36:23Z  \n",
       "6   2023-05-09T11:05:01Z  \n",
       "7   2023-04-10T14:41:11Z  \n",
       "8   2024-03-29T16:27:25Z  \n",
       "9   2023-03-28T07:42:11Z  \n",
       "10  2023-07-11T08:35:41Z  \n",
       "11  2024-03-08T12:07:44Z  \n",
       "12  2023-07-20T13:37:09Z  \n",
       "13  2024-05-09T15:36:36Z  \n",
       "14  2023-12-04T13:48:14Z  \n",
       "15  2023-06-08T16:21:33Z  \n",
       "16  2023-12-19T14:28:41Z  \n",
       "17  2023-07-27T09:02:32Z  \n",
       "18  2023-07-13T15:21:58Z  \n",
       "19  2025-07-23T17:07:06Z  \n",
       "20  2023-08-24T15:27:21Z  \n",
       "21  2024-05-15T07:16:39Z  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a docket to analyze\n",
    "docket_id = \"EPA-HQ-OAR-2021-0317\"  # Change to your target docket\n",
    "\n",
    "# Get docket info\n",
    "docket_info = conn.execute(f\"\"\"\n",
    "    SELECT docket_id, agency_code, title, docket_type, modify_date\n",
    "    FROM read_parquet('{R2_BASE_URL}/dockets.parquet')\n",
    "    WHERE docket_id = '{docket_id}'\n",
    "\"\"\").fetchdf()\n",
    "docket_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Comment Volume Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docket Analysis: EPA-HQ-OAR-2021-0317\n",
      "==================================================\n",
      "Total comments: 3,639\n",
      "Unique texts: 1,120\n",
      "Comment period: 2021-11-17T05:00:00Z to 2025-07-23T04:00:00Z\n",
      "Average comment length: 335 chars\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "stats = conn.execute(f\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) as total_comments,\n",
    "        COUNT(DISTINCT comment) as unique_texts,\n",
    "        MIN(posted_date) as first_comment,\n",
    "        MAX(posted_date) as last_comment,\n",
    "        AVG(LENGTH(comment)) as avg_length\n",
    "    FROM read_parquet('{R2_BASE_URL}/comments.parquet')\n",
    "    WHERE docket_id = '{docket_id}'\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(f\"Docket Analysis: {docket_id}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total comments: {stats['total_comments'].iloc[0]:,}\")\n",
    "print(f\"Unique texts: {stats['unique_texts'].iloc[0]:,}\")\n",
    "print(f\"Comment period: {stats['first_comment'].iloc[0]} to {stats['last_comment'].iloc[0]}\")\n",
    "print(f\"Average comment length: {stats['avg_length'].iloc[0]:.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Timeline Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Comment Volume:\n",
      "------------------------------------------------------------\n",
      "2023-03-06 00:00:00 | ██ 32\n",
      "2023-03-08 00:00:00 | █ 20\n",
      "2023-03-28 00:00:00 | █████ 70\n",
      "2023-03-30 00:00:00 | ██ 25\n",
      "2023-04-03 00:00:00 |  1\n",
      "2023-04-07 00:00:00 | █████████ 114\n",
      "2023-04-10 00:00:00 | █████████████████████ 264\n",
      "2023-04-12 00:00:00 | ████████████████████████████████████████ 489\n",
      "2023-04-13 00:00:00 | ████ 55\n",
      "2023-05-09 00:00:00 |  1\n",
      "2023-06-08 00:00:00 |  2\n",
      "2023-07-11 00:00:00 |  2\n",
      "2023-07-13 00:00:00 |  1\n",
      "2023-07-20 00:00:00 |  4\n",
      "2023-07-27 00:00:00 | ███ 44\n",
      "2023-08-02 00:00:00 |  1\n",
      "2023-08-15 00:00:00 |  5\n",
      "2023-08-24 00:00:00 |  1\n",
      "2023-12-04 00:00:00 |  4\n",
      "2023-12-07 00:00:00 |  2\n",
      "2023-12-19 00:00:00 |  2\n",
      "2024-05-07 00:00:00 |  6\n",
      "2024-05-09 00:00:00 | █ 18\n",
      "2024-05-15 00:00:00 | ███ 40\n",
      "2024-05-20 00:00:00 |  2\n",
      "2024-08-28 00:00:00 |  4\n",
      "2024-09-04 00:00:00 |  1\n",
      "2024-09-09 00:00:00 |  1\n",
      "2024-10-23 00:00:00 |  1\n",
      "2025-07-23 00:00:00 |  1\n"
     ]
    }
   ],
   "source": [
    "# Daily comment volume\n",
    "timeline = conn.execute(f\"\"\"\n",
    "    SELECT \n",
    "        CAST(posted_date AS DATE) as date,\n",
    "        COUNT(*) as comments\n",
    "    FROM read_parquet('{R2_BASE_URL}/comments.parquet')\n",
    "    WHERE docket_id = '{docket_id}'\n",
    "      AND posted_date IS NOT NULL\n",
    "    GROUP BY CAST(posted_date AS DATE)\n",
    "    ORDER BY date\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "# Simple ASCII chart\n",
    "max_comments = timeline['comments'].max()\n",
    "print(\"Daily Comment Volume:\")\n",
    "print(\"-\" * 60)\n",
    "for _, row in timeline.tail(30).iterrows():\n",
    "    bar_length = int(40 * row['comments'] / max_comments)\n",
    "    print(f\"{row['date']} | {'█' * bar_length} {row['comments']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comment Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_category</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Very short (&lt;100 chars)</td>\n",
       "      <td>2676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medium (500-2000 chars)</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Short (100-500 chars)</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Long (2000-10000 chars)</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           length_category  count\n",
       "0  Very short (<100 chars)   2676\n",
       "1  Medium (500-2000 chars)    448\n",
       "2    Short (100-500 chars)    332\n",
       "3  Long (2000-10000 chars)    181"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categorize by length (proxy for effort/detail)\n",
    "length_dist = conn.execute(f\"\"\"\n",
    "    SELECT\n",
    "        CASE \n",
    "            WHEN LENGTH(comment) < 100 THEN 'Very short (<100 chars)'\n",
    "            WHEN LENGTH(comment) < 500 THEN 'Short (100-500 chars)'\n",
    "            WHEN LENGTH(comment) < 2000 THEN 'Medium (500-2000 chars)'\n",
    "            WHEN LENGTH(comment) < 10000 THEN 'Long (2000-10000 chars)'\n",
    "            ELSE 'Very long (10000+ chars)'\n",
    "        END as length_category,\n",
    "        COUNT(*) as count\n",
    "    FROM read_parquet('{R2_BASE_URL}/comments.parquet')\n",
    "    WHERE docket_id = '{docket_id}'\n",
    "      AND comment IS NOT NULL\n",
    "    GROUP BY length_category\n",
    "    ORDER BY count DESC\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "length_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sample Unique vs Duplicate Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Unique Comments (likely individual):\n",
      "\n",
      "--- EPA-HQ-OAR-2021-0317-0206 ---\n",
      "With energy costs rising almost daily this new regulation needs to be looked at thoroughly. We can control harmful emissions better with innovation instead of more regulation. At this point in time I ...\n",
      "\n",
      "--- EPA-HQ-OAR-2021-0317-0202 ---\n",
      "Hello,<br/><br/>I am new to this process but as I see the world changing and the risk of our own human extinction increasing everyday, I feel the need to start getting more involved. I&rsquo;m not a s...\n",
      "\n",
      "--- EPA-HQ-OAR-2021-0317-0208 ---\n",
      "The EPA has long been an enforcement agency of the radical left. Time and time again their &quot;recommendations&quot; have been nothing more than the furthering of a radical left-wing agenda on clima...\n",
      "\n",
      "--- EPA-HQ-OAR-2021-0317-0207 ---\n",
      "I believe methane gas is produced from decomposition of organic material and is thus naturally occurring. Is it considered &quot;green&quot; energy?  It is produced in our bodies and in the bodies of ...\n",
      "\n",
      "--- EPA-HQ-OAR-2021-0317-0212 ---\n",
      "What do you think you are saving? The planet? You aren&#39;t saving anything. Why don&#39;t you do some good and make Logging Illegal? Why don&#39;t you start doing that. All of your Law&#39;s you thi...\n"
     ]
    }
   ],
   "source": [
    "# Sample of unique, substantive comments (likely individual submissions)\n",
    "unique_comments = conn.execute(f\"\"\"\n",
    "    WITH comment_counts AS (\n",
    "        SELECT comment, COUNT(*) as cnt\n",
    "        FROM read_parquet('{R2_BASE_URL}/comments.parquet')\n",
    "        WHERE docket_id = '{docket_id}'\n",
    "          AND comment IS NOT NULL\n",
    "          AND LENGTH(comment) > 200\n",
    "        GROUP BY comment\n",
    "    )\n",
    "    SELECT c.comment_id, c.title, LEFT(c.comment, 300) as comment_preview\n",
    "    FROM read_parquet('{R2_BASE_URL}/comments.parquet') c\n",
    "    JOIN comment_counts cc ON c.comment = cc.comment\n",
    "    WHERE c.docket_id = '{docket_id}'\n",
    "      AND cc.cnt = 1\n",
    "    LIMIT 5\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(\"Sample Unique Comments (likely individual):\")\n",
    "for _, row in unique_comments.iterrows():\n",
    "    print(f\"\\n--- {row['comment_id']} ---\")\n",
    "    print(row['comment_preview'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Phrases (Simple Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Keywords:\n",
      "  methane: 574\n",
      "  attached: 279\n",
      "  pollution: 245\n",
      "  emissions: 221\n",
      "  climate: 214\n",
      "  wells: 177\n",
      "  health: 170\n",
      "  rules: 154\n",
      "  more: 150\n",
      "  rsquo: 148\n",
      "  proposed: 146\n",
      "  change: 125\n",
      "  industry: 122\n",
      "  flaring: 115\n",
      "  their: 110\n",
      "  also: 110\n",
      "  protect: 103\n",
      "  span: 101\n",
      "  monitoring: 99\n",
      "  energy: 99\n"
     ]
    }
   ],
   "source": [
    "# Get sample comments for keyword extraction\n",
    "sample = conn.execute(f\"\"\"\n",
    "    SELECT comment\n",
    "    FROM read_parquet('{R2_BASE_URL}/comments.parquet')\n",
    "    WHERE docket_id = '{docket_id}'\n",
    "      AND comment IS NOT NULL\n",
    "    LIMIT 500\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "# Simple word frequency (excluding common words)\n",
    "stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'is', 'are', 'was', 'were', 'be', 'been', 'this', 'that', 'these', 'those', 'i', 'we', 'you', 'it', 'they', 'my', 'our', 'your', 'as', 'not', 'have', 'has', 'will', 'would', 'should', 'could', 'can', 'all', 'any', 'if', 'so', 'do', 'does'}\n",
    "\n",
    "all_words = []\n",
    "for text in sample['comment'].dropna():\n",
    "    words = re.findall(r'\\b[a-z]{4,}\\b', text.lower())\n",
    "    all_words.extend([w for w in words if w not in stopwords])\n",
    "\n",
    "print(\"Top 20 Keywords:\")\n",
    "for word, count in Counter(all_words).most_common(20):\n",
    "    print(f\"  {word}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
